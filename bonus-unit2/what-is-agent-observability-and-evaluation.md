# AI 에이전트 관찰 가능성 및 평가

## 🔎 관찰 가능성이란 무엇인가?

관찰 가능성은 로그, 메트릭, 추적과 같은 외부 신호를 통해 AI 에이전트 내부에서 무슨 일이 일어나고 있는지 이해하는 것입니다. AI 에이전트의 경우 이는 에이전트 성능을 디버깅하고 개선하기 위해 작업, 도구 사용, 모델 호출 및 응답을 추적하는 것을 의미합니다.

![관찰 가능성 대시보드](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## 🔭 에이전트 관찰 가능성이 중요한 이유

관찰 가능성이 없으면 AI 에이전트는 "블랙박스"입니다. 관찰 가능성 도구는 에이전트를 투명하게 만들어 다음을 가능하게 합니다.

- 비용과 정확성의 절충안 이해
- 대기 시간 측정
- 유해 언어 및 프롬프트 주입 감지
- 사용자 피드백 모니터링

즉, 데모 에이전트를 프로덕션에 사용할 수 있도록 준비합니다!

## 🔨 관찰 가능성 도구

AI 에이전트를 위한 일반적인 관찰 가능성 도구에는 [Langfuse](https://langfuse.com) 및 [Arize](https://www.arize.com)와 같은 플랫폼이 포함됩니다. 이러한 도구는 상세한 추적을 수집하고 실시간으로 메트릭을 모니터링하는 대시보드를 제공하여 문제를 쉽게 감지하고 성능을 최적화할 수 있도록 합니다.

관찰 가능성 도구는 기능과 기능 면에서 매우 다양합니다. 일부 도구는 오픈 소스이며, 로드맵과 광범위한 통합을 형성하는 대규모 커뮤니티의 이점을 누립니다. 또한 특정 도구는 관찰 가능성, 평가 또는 프롬프트 관리와 같은 LLMOps의 특정 측면에 특화되어 있는 반면, 다른 도구는 전체 LLMOps 워크플로를 포괄하도록 설계되었습니다. 자신에게 적합한 솔루션을 선택하기 위해 다양한 옵션의 설명서를 살펴보는 것이 좋습니다.

[smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index)와 같은 많은 에이전트 프레임워크는 [OpenTelemetry](https://opentelemetry.io/docs/) 표준을 사용하여 관찰 가능성 도구에 메타데이터를 노출합니다. 이 외에도 관찰 가능성 도구는 빠르게 변화하는 LLM 세계에서 더 많은 유연성을 허용하기 위해 사용자 지정 계측을 구축합니다. 지원되는 내용을 확인하려면 사용 중인 도구의 설명서를 확인해야 합니다.

## 🔬추적 및 범위

관찰 가능성 도구는 일반적으로 에이전트 실행을 추적 및 범위로 나타냅니다.

- **추적**은 처음부터 끝까지 완전한 에이전트 작업을 나타냅니다(예: 사용자 쿼리 처리).
- **범위**는 추적 내의 개별 단계입니다(예: 언어 모델 호출 또는 데이터 검색).

![Langfuse의 smolagent 추적 예](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## 📊 모니터링할 주요 메트릭

다음은 관찰 가능성 도구가 모니터링하는 가장 일반적인 메트릭 중 일부입니다.

**대기 시간:** 에이전트가 얼마나 빨리 응답합니까? 대기 시간이 길면 사용자 경험에 부정적인 영향을 미칩니다. 에이전트 실행을 추적하여 작업 및 개별 단계의 대기 시간을 측정해야 합니다. 예를 들어 모든 모델 호출에 20초가 걸리는 에이전트는 더 빠른 모델을 사용하거나 모델 호출을 병렬로 실행하여 가속화할 수 있습니다.

**비용:** 에이전트 실행당 비용은 얼마입니까? AI 에이전트는 토큰당 또는 외부 API로 청구되는 LLM 호출에 의존합니다. 잦은 도구 사용 또는 여러 프롬프트는 비용을 급격히 증가시킬 수 있습니다. 예를 들어 에이전트가 한계 품질 향상을 위해 LLM을 5번 호출하는 경우 비용이 정당한지 또는 호출 횟수를 줄이거나 더 저렴한 모델을 사용할 수 있는지 평가해야 합니다. 실시간 모니터링은 예상치 못한 급증(예: 과도한 API 루프를 유발하는 버그)을 식별하는 데도 도움이 될 수 있습니다.

**요청 오류:** 에이전트가 실패한 요청은 몇 개입니까? 여기에는 API 오류 또는 실패한 도구 호출이 포함될 수 있습니다. 프로덕션에서 이러한 오류에 대해 에이전트를 더 강력하게 만들려면 대체 또는 재시도를 설정할 수 있습니다. 예를 들어 LLM 공급자 A가 다운되면 백업으로 LLM 공급자 B로 전환합니다.

**사용자 피드백:** 직접적인 사용자 평가를 구현하면 귀중한 통찰력을 얻을 수 있습니다. 여기에는 명시적인 평가(👍좋아요/👎싫어요, ⭐1-5개) 또는 텍스트 댓글이 포함될 수 있습니다. 일관된 부정적인 피드백은 에이전트가 예상대로 작동하지 않는다는 신호이므로 경고해야 합니다.

**암시적 사용자 피드백:** 사용자 행동은 명시적인 평가 없이도 간접적인 피드백을 제공합니다. 여기에는 즉각적인 질문 재구성, 반복적인 쿼리 또는 재시도 버튼 클릭이 포함될 수 있습니다. 예를 들어 사용자가 반복적으로 동일한 질문을 하는 것을 보면 에이전트가 예상대로 작동하지 않는다는 신호입니다.

**정확성:** 에이전트가 정확하거나 바람직한 출력을 얼마나 자주 생성합니까? 정확성 정의는 다양합니다(예: 문제 해결 정확성, 정보 검색 정확성, 사용자 만족도). 첫 번째 단계는 에이전트의 성공이 무엇인지 정의하는 것입니다. 자동화된 확인, 평가 점수 또는 작업 완료 레이블을 통해 정확성을 추적할 수 있습니다. 예를 들어 추적을 "성공" 또는 "실패"로 표시합니다.

**자동화된 평가 메트릭:** 자동화된 평가를 설정할 수도 있습니다. 예를 들어 LLM을 사용하여 에이전트의 출력이 유용하거나 정확한지 여부를 채점할 수 있습니다. 에이전트의 다양한 측면을 채점하는 데 도움이 되는 여러 오픈 소스 라이브러리도 있습니다. 예를 들어 RAG 에이전트용 [RAGAS](https://docs.ragas.io/) 또는 유해 언어 또는 프롬프트 주입을 감지하는 [LLM Guard](https://llm-guard.com/)가 있습니다.

실제로 이러한 메트릭의 조합은 AI 에이전트의 상태를 가장 잘 파악할 수 있습니다. 이 장의 [예제 노트북](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb)에서는 이러한 메트릭이 실제 예제에서 어떻게 보이는지 보여 드리지만 먼저 일반적인 평가 워크플로가 어떻게 보이는지 알아보겠습니다.

## 👍 AI 에이전트 평가

관찰 가능성은 우리에게 메트릭을 제공하지만 평가는 AI 에이전트가 얼마나 잘 수행되고 있는지, 어떻게 개선될 수 있는지 판단하기 위해 해당 데이터를 분석하고 테스트를 수행하는 프로세스입니다. 즉, 이러한 추적 및 메트릭이 있으면 에이전트를 판단하고 결정을 내리는 데 어떻게 사용합니까?

AI 에이전트는 종종 비결정적이며 업데이트 또는 모델 동작 변화를 통해 진화할 수 있기 때문에 정기적인 평가가 중요합니다. 평가 없이는 "스마트 에이전트"가 실제로 작업을 잘 수행하고 있는지 또는 퇴보했는지 알 수 없습니다.

AI 에이전트에 대한 평가는 **온라인 평가**와 **오프라인 평가**의 두 가지 범주가 있습니다. 둘 다 가치가 있으며 서로를 보완합니다. 일반적으로 오프라인 평가부터 시작합니다. 이는 에이전트를 배포하기 전에 필요한 최소한의 단계이기 때문입니다.

### 🥷 오프라인 평가

![Langfuse의 데이터 세트 항목](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

이는 일반적으로 테스트 데이터 세트를 사용하여 제어된 환경에서 에이전트를 평가하는 것을 포함하며 라이브 사용자 쿼리는 포함하지 않습니다. 예상 출력 또는 올바른 동작을 알고 있는 선별된 데이터 세트를 사용한 다음 해당 데이터 세트에서 에이전트를 실행합니다.

예를 들어 수학 단어 문제 에이전트를 구축한 경우 알려진 답변이 있는 100개의 문제로 구성된 [테스트 데이터 세트](https://huggingface.co/datasets/gsm8k)가 있을 수 있습니다. 오프라인 평가는 개선 사항을 확인하거나 회귀를 방지하기 위해 개발 중에 수행되는 경우가 많으며 CI/CD 파이프라인의 일부가 될 수 있습니다. 이점은 **반복 가능하고 정답이 있으므로 명확한 정확도 메트릭을 얻을 수 있다는 것**입니다. 또한 사용자 쿼리를 시뮬레이션하고 이상적인 답변에 대한 에이전트의 응답을 측정하거나 위에서 설명한 대로 자동화된 메트릭을 사용할 수도 있습니다.

오프라인 평가의 주요 과제는 테스트 데이터 세트가 포괄적이고 관련성을 유지하도록 하는 것입니다. 에이전트는 고정된 테스트 세트에서는 잘 수행될 수 있지만 프로덕션에서는 매우 다른 쿼리를 접할 수 있습니다. 따라서 실제 시나리오를 반영하는 새로운 엣지 케이스와 예제로 테스트 세트를 계속 업데이트해야 합니다. 작은 "스모크 테스트" 사례와 더 큰 평가 세트를 혼합하는 것이 유용합니다. 빠른 확인을 위한 작은 세트와 더 넓은 성능 메트릭을 위한 더 큰 세트입니다.

### 🔄 온라인 평가

이는 라이브, 실제 환경, 즉 프로덕션에서 실제 사용 중에 에이전트를 평가하는 것을 의미합니다. 온라인 평가는 실제 사용자 상호 작용에 대한 에이전트의 성능을 모니터링하고 결과를 지속적으로 분석하는 것을 포함합니다.

예를 들어 라이브 트래픽에서 성공률, 사용자 만족도 점수 또는 기타 메트릭을 추적할 수 있습니다. 온라인 평가의 장점은 **실험실 환경에서 예상하지 못할 수 있는 것을 포착할 수 있다는 것**입니다. 시간 경과에 따른 모델 드리프트(입력 패턴이 변경됨에 따라 에이전트의 효율성이 저하되는 경우)를 관찰하고 테스트 데이터에 없었던 예기치 않은 쿼리 또는 상황을 포착할 수 있습니다. 이는 에이전트가 실제 환경에서 어떻게 작동하는지에 대한 진정한 그림을 제공합니다.

온라인 평가는 종종 논의된 바와 같이 암시적 및 명시적 사용자 피드백을 수집하고 잠재적으로 섀도 테스트 또는 A/B 테스트(새 버전의 에이전트가 이전 버전과 비교하기 위해 병렬로 실행되는 경우)를 실행하는 것을 포함합니다. 과제는 라이브 상호 작용에 대한 신뢰할 수 있는 레이블이나 점수를 얻는 것이 까다로울 수 있다는 것입니다. 사용자 피드백이나 다운스트림 메트릭(예: 사용자가 결과를 클릭했는지 여부)에 의존할 수 있습니다.

### 🤝 두 가지 결합

실제로 성공적인 AI 에이전트 평가는 **온라인** 및 **오프라인** 방법을 혼합합니다. 정기적인 오프라인 벤치마크를 실행하여 정의된 작업에 대해 에이전트를 정량적으로 채점하고 라이브 사용을 지속적으로 모니터링하여 벤치마크가 놓친 것을 포착할 수 있습니다. 예를 들어 오프라인 테스트는 코드 생성 에이전트의 알려진 문제 세트에 대한 성공률이 향상되고 있는지 포착할 수 있는 반면, 온라인 모니터링은 사용자가 에이전트가 어려움을 겪는 새로운 범주의 질문을 하기 시작했다는 것을 경고할 수 있습니다. 둘 다 결합하면 더 강력한 그림을 얻을 수 있습니다.

실제로 많은 팀이 _오프라인 평가 → 새 에이전트 버전 배포 → 온라인 메트릭 모니터링 및 새 실패 예제 수집 → 해당 예제를 오프라인 테스트 세트에 추가 → 반복_이라는 루프를 채택합니다. 이런 식으로 평가는 지속적이고 계속 개선됩니다.

## 🧑‍💻 실제로 어떻게 작동하는지 봅시다

다음 섹션에서는 관찰 가능성 도구를 사용하여 에이전트를 모니터링하고 평가하는 방법에 대한 예를 살펴보겠습니다.