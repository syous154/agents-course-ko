# LlamaIndexì—ì„œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ìƒì„±í•˜ê¸°

LlamaIndexì˜ ì›Œí¬í”Œë¡œìš°ëŠ” ì½”ë“œë¥¼ ìˆœì°¨ì ì´ê³  ê´€ë¦¬ ê°€ëŠ¥í•œ ë‹¨ê³„ë¡œ êµ¬ì„±í•˜ëŠ” êµ¬ì¡°í™”ëœ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ì›Œí¬í”Œë¡œìš°ëŠ” `Events`ì— ì˜í•´ íŠ¸ë¦¬ê±°ë˜ê³ , ìŠ¤ìŠ¤ë¡œ `Events`ë¥¼ ë°©ì¶œí•˜ì—¬ ì¶”ê°€ ë‹¨ê³„ë¥¼ íŠ¸ë¦¬ê±°í•˜ëŠ” `Steps`ë¥¼ ì •ì˜í•¨ìœ¼ë¡œì¨ ìƒì„±ë©ë‹ˆë‹¤.
Alfredê°€ RAG ì‘ì—…ì„ ìœ„í•œ LlamaIndex ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

![Workflow Schematic](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflows.png)

**ì›Œí¬í”Œë¡œìš°ëŠ” ëª‡ ê°€ì§€ ì£¼ìš” ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤:**

- ì½”ë“œë¥¼ ê°œë³„ ë‹¨ê³„ë¡œ ëª…í™•í•˜ê²Œ êµ¬ì„±
- ìœ ì—°í•œ ì œì–´ íë¦„ì„ ìœ„í•œ ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜
- ë‹¨ê³„ ê°„ íƒ€ì… ì•ˆì „ í†µì‹ 
- ë‚´ì¥ëœ ìƒíƒœ ê´€ë¦¬
- ê°„ë‹¨í•˜ê³  ë³µì¡í•œ ì—ì´ì „íŠ¸ ìƒí˜¸ ì‘ìš© ëª¨ë‘ ì§€ì›

ì§ì‘í•˜ì…¨ê² ì§€ë§Œ, **ì›Œí¬í”Œë¡œìš°ëŠ” ì—ì´ì „íŠ¸ì˜ ììœ¨ì„±ê³¼ ì „ì²´ ì›Œí¬í”Œë¡œìš°ì— ëŒ€í•œ ì œì–´ ì‚¬ì´ì—ì„œ í›Œë¥­í•œ ê· í˜•ì„ ì´ë£¹ë‹ˆë‹¤.**

ê·¸ëŸ¼, ì§ì ‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!

## ì›Œí¬í”Œë¡œìš° ìƒì„±í•˜ê¸°

<Tip>
<a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/workflows.ipynb" target="_blank">ì´ ë…¸íŠ¸ë¶</a>ì˜ ì½”ë“œë¥¼ ë”°ë¼ í•  ìˆ˜ ìˆìœ¼ë©°, Google Colabì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</Tip>

### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ìƒì„±

<details>
<summary>ì›Œí¬í”Œë¡œìš° íŒ¨í‚¤ì§€ ì„¤ì¹˜</summary>
<a href="./llama-hub">LlamaHub ì„¹ì…˜</a>ì—ì„œ ì†Œê°œëœ ë°”ì™€ ê°™ì´, ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš° íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
pip install llama-index-utils-workflow
```
</details>

`Workflow`ë¥¼ ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  `@step` ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ë¥¼ ì¥ì‹í•˜ì—¬ ë‹¨ì¼ ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë˜í•œ ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ê³¼ ëì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜ ì´ë²¤íŠ¸ì¸ `StartEvent`ì™€ `StopEvent`ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step

class MyWorkflow(Workflow):
    @step
    async def my_step(self, ev: StartEvent) -> StopEvent:
        # do something here
        return StopEvent(result="Hello, world!")


w = MyWorkflow(timeout=10, verbose=False)
result = await w.run()
```

ë³´ì‹œë‹¤ì‹œí”¼, ì´ì œ `w.run()`ì„ í˜¸ì¶œí•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì—¬ëŸ¬ ë‹¨ê³„ ì—°ê²°í•˜ê¸°

ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì—°ê²°í•˜ë ¤ë©´ **ë‹¨ê³„ ê°„ì— ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ì‚¬ìš©ì ì§€ì • ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
ì´ë¥¼ ìœ„í•´ ë‹¨ê³„ ê°„ì— ì „ë‹¬ë˜ì–´ ì²« ë²ˆì§¸ ë‹¨ê³„ì˜ ì¶œë ¥ì„ ë‘ ë²ˆì§¸ ë‹¨ê³„ë¡œ ì „ë‹¬í•˜ëŠ” `Event`ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from llama_index.core.workflow import Event

class ProcessingEvent(Event):
    intermediate_result: str

class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:
        # Process initial data
        return ProcessingEvent(intermediate_result="Step 1 complete")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)

w = MultiStepWorkflow(timeout=10, verbose=False)
result = await w.run()
result
```

ì—¬ê¸°ì„œ íƒ€ì… íŒíŒ…ì€ ì›Œí¬í”Œë¡œìš°ê°€ ì˜¬ë°”ë¥´ê²Œ ì‹¤í–‰ë˜ë„ë¡ ë³´ì¥í•˜ë¯€ë¡œ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ì œ ì¢€ ë” ë³µì¡í•˜ê²Œ ë§Œë“¤ì–´ ë´…ì‹œë‹¤!

### ë£¨í”„ ë° ë¶„ê¸°

íƒ€ì… íŒíŒ…ì€ ì›Œí¬í”Œë¡œìš°ì˜ ê°€ì¥ ê°•ë ¥í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¶„ê¸°, ë£¨í”„ ë° ì¡°ì¸ì„ ìƒì„±í•˜ì—¬ ë” ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìš©ì´í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•©ì§‘í•© ì—°ì‚°ì `|`ë¥¼ ì‚¬ìš©í•˜ì—¬ **ë£¨í”„ë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì‹œ**ë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ì•„ë˜ ì˜ˆì‹œì—ì„œ `LoopEvent`ëŠ” ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©° ì¶œë ¥ìœ¼ë¡œë„ ë°˜í™˜ë  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from llama_index.core.workflow import Event
import random


class ProcessingEvent(Event):
    intermediate_result: str


class LoopEvent(Event):
    loop_output: str


class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:
        if random.randint(0, 1) == 0:
            print("Bad thing happened")
            return LoopEvent(loop_output="Back to step one.")
        else:
            print("Good thing happened")
            return ProcessingEvent(intermediate_result="First step complete.")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)


w = MultiStepWorkflow(verbose=False)
result = await w.run()
result
```

### ì›Œí¬í”Œë¡œìš° ê·¸ë¦¬ê¸°

ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. `draw_all_possible_flows` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë ¤ë´…ì‹œë‹¤. ì´ í•¨ìˆ˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```python
from llama_index.utils.workflow import draw_all_possible_flows

w = ... # as defined in the previous section
draw_all_possible_flows(w, "flow.html")
```

![workflow drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)

ê³¼ì •ì—ì„œ ë‹¤ë£° ë§ˆì§€ë§‰ ë©‹ì§„ íŠ¸ë¦­ì€ ì›Œí¬í”Œë¡œìš°ì— ìƒíƒœë¥¼ ì¶”ê°€í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### ìƒíƒœ ê´€ë¦¬

ìƒíƒœ ê´€ë¦¬ëŠ” ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœë¥¼ ì¶”ì í•˜ì—¬ ëª¨ë“  ë‹¨ê³„ê°€ ë™ì¼í•œ ìƒíƒœì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.
ì´ëŠ” ë‹¨ê³„ í•¨ìˆ˜ì—ì„œ ë§¤ê°œë³€ìˆ˜ ìœ„ì— `Context` íƒ€ì… íŒíŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from llama_index.core.workflow import Context, StartEvent, StopEvent


@step
async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:
    # store query in the context
    await ctx.store.set("query", "What is the capital of France?")

    # do something with context and event
    val = ...

    # retrieve query from the context
    query = await ctx.store.get("query")

    return StopEvent(result=val)
```

í›Œë¥­í•©ë‹ˆë‹¤! ì´ì œ LlamaIndexì—ì„œ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤!

<Tip>ì›Œí¬í”Œë¡œìš°ì—ëŠ” ë” ë³µì¡í•œ ë‰˜ì•™ìŠ¤ê°€ ìˆìœ¼ë©°, <a href="https://docs.llamaindex.ai/en/stable/understanding/workflows/">LlamaIndex ë¬¸ì„œ</a>ì—ì„œ ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</Tip>

í•˜ì§€ë§Œ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì´ ìˆëŠ”ë°, ì´ëŠ” `AgentWorkflow` í´ë˜ìŠ¤ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¡œ ì›Œí¬í”Œë¡œìš° ìë™í™”í•˜ê¸°

ìˆ˜ë™ ì›Œí¬í”Œë¡œìš° ìƒì„± ëŒ€ì‹ , **`AgentWorkflow` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
`AgentWorkflow`ëŠ” ì›Œí¬í”Œë¡œìš° ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸í™”ëœ ê¸°ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì„œë¡œ í˜‘ë ¥í•˜ê³  ì‘ì—…ì„ ì¸ê³„í•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ ì´ìƒì˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì˜ ë‹¤ë¥¸ ì¸¡ë©´ì„ ì²˜ë¦¬í•˜ëŠ” ë³µì¡í•œ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
`llama_index.core.agent`ì—ì„œ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ëŒ€ì‹ , `llama_index.core.agent.workflow`ì—ì„œ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì…ë‹ˆë‹¤.
`AgentWorkflow` ìƒì„±ìì—ì„œ í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ë¥¼ ë£¨íŠ¸ ì—ì´ì „íŠ¸ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë“¤ì–´ì˜¤ë©´ ë¨¼ì € ë£¨íŠ¸ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…ë©ë‹ˆë‹¤.

ê° ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì²­ì„ ì§ì ‘ ì²˜ë¦¬
- ì‘ì—…ì— ë” ì í•©í•œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ì¸ê³„
- ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ ë°˜í™˜

ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```python
from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Define some tools
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description
multiply_agent = ReActAgent(
    name="multiply_agent",
    description="Is able to multiply two integers",
    system_prompt="A helpful assistant that can use a tool to multiply numbers.",
    tools=[multiply],
    llm=llm,
)

addition_agent = ReActAgent(
    name="add_agent",
    description="Is able to add two integers",
    system_prompt="A helpful assistant that can use a tool to add numbers.",
    tools=[add],
    llm=llm,
)

# Create the workflow
workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
)

# Run the system
response = await workflow.run(user_msg="Can you add 5 and 3?")
```

ì—ì´ì „íŠ¸ ë„êµ¬ëŠ” ì•ì„œ ì–¸ê¸‰í•œ ì›Œí¬í”Œë¡œìš° ìƒíƒœë¥¼ ìˆ˜ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ˆê¸° ìƒíƒœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìƒíƒœëŠ” ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ì˜ ìƒíƒœ í‚¤ì— ì €ì¥ë©ë‹ˆë‹¤. ì´ëŠ” ê° ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¦ê°•í•˜ëŠ” `state_prompt`ì— ì£¼ì…ë©ë‹ˆë‹¤.

ì´ì „ ì˜ˆì‹œë¥¼ ìˆ˜ì •í•˜ì—¬ í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì„¸ëŠ” ì¹´ìš´í„°ë¥¼ ì£¼ì…í•´ ë´…ì‹œë‹¤.

```python
from llama_index.core.workflow import Context

# Define some tools
async def add(ctx: Context, a: int, b: int) -> int:
    """Add two numbers."""
    # update our count
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a * b

async def multiply(ctx: Context, a: int, b: int) -> int:
    """Multiply two numbers."""
    # update our count
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a * b

...

workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
    initial_state={"num_fn_calls": 0},
    state_prompt="Current state: {state}. User message: {msg}",
)

# run the workflow with context
ctx = Context(workflow)
response = await workflow.run(user_msg="Can you add 5 and 3?")

# pull out and inspect the state
state = await ctx.store.get("state")
print(state["num_fn_calls"])
```

ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ LlamaIndexì˜ ì—ì´ì „íŠ¸ ê¸°ë³¸ ì‚¬í•­ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ğŸ‰

ì§€ì‹ì„ êµ³ê±´íˆ í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ í€´ì¦ˆë¥¼ í’€ì–´ë´…ì‹œë‹¤! ğŸš€